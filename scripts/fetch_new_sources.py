"""
Pobiera content z nowych ≈∫r√≥de≈Ç - Faza 1.
Wykorzystuje istniejƒÖcy rss_fetcher.py ale tylko dla nowych ≈∫r√≥de≈Ç.
"""
import sys
from pathlib import Path

# Add project root to path
ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT))

from ingest.rss_fetcher import fetch_feed, rate_limit_sleep, slugify
import json

# Nowe ≈∫r√≥d≈Ça do scrapowania (dzia≈ÇajƒÖce)
NEW_SOURCES = [
    {
        "name": "Independent Institute",
        "feed": "https://www.independent.org/feed/",
        "type": "opinion",
        "country": "US", 
        "license": "permissive",
        "rate_limit_rps": 0.2
    },
    {
        "name": "AIER",
        "feed": "https://www.aier.org/feed/", 
        "type": "opinion",
        "country": "US",
        "license": "permissive",
        "rate_limit_rps": 0.2
    },
    {
        "name": "Niezalezna",
        "feed": "https://niezalezna.pl/feed/",
        "type": "opinion", 
        "country": "PL",
        "license": "permissive",
        "rate_limit_rps": 0.1
    }
]

def fetch_new_sources():
    """Pobiera artyku≈Çy z nowych ≈∫r√≥de≈Ç."""
    print("=== POBIERANIE NOWYCH ≈πR√ìDE≈Å - FAZA 1 ===\n")
    
    total_fetched = 0
    
    for source in NEW_SOURCES:
        print(f"Fetching: {source['name']}")
        print(f"URL: {source['feed']}")
        
        try:
            # Fetch articles from this source 
            articles = list(fetch_feed(
                url=source['feed'],
                user_agent="SatyrAI-bot/0.1"
            ))
            
            if articles:
                print(f"  ‚úÖ Pobrano: {len(articles)} artyku≈Ç√≥w")
                total_fetched += len(articles)
                
                # Dodaj metadane do ka≈ºdego artyku≈Çu
                enriched_articles = []
                for article in articles:
                    enriched = {
                        "source": source['name'],
                        "feed": source['feed'],
                        "type": source['type'],
                        "country": source['country'],
                        "license": source['license'],
                        "data": article
                    }
                    enriched_articles.append(enriched)
                
                # Save to separate file for new sources
                slug = slugify(source['name'])
                output_file = ROOT / "data" / "raw" / f"new-{slug}.jsonl"
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                with output_file.open('w', encoding='utf-8') as f:
                    for enriched in enriched_articles:
                        f.write(json.dumps(enriched, ensure_ascii=False) + '\n')
                
                print(f"  üíæ Zapisano do: {output_file}")
            else:
                print(f"  ‚ùå Brak artyku≈Ç√≥w")
                
        except Exception as e:
            print(f"  ‚ùå B≈ÇƒÖd: {e}")
            
        # Rate limiting
        rate_limit_sleep(source['rate_limit_rps'])
        print()
    
    print(f"=== PODSUMOWANIE ===")
    print(f"≈ÅƒÖcznie pobrano: {total_fetched} nowych artyku≈Ç√≥w")
    print(f"Lokalizacja: {ROOT}/data/raw/new-*.jsonl")
    
    return total_fetched

if __name__ == "__main__":
    fetch_new_sources()